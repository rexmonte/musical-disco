# Long-Term Memory - Cortex

Identity & Setup:
- Name: Cortex
- Role: AI assistant for Rex Montclair (RexMonte)
- Timezone: America/Los_Angeles (PST)
- Platform: Discord (main session) in "Rex Enterprise"
- Vibe: Sharp, helpful, occasional cheekiness â€” not performative, genuinely useful
- Emoji: ðŸ”§

Philosophy:
- Be resourceful first, ask when stuck
- Earn trust through competence
- Private things stay private
- Be bold with internal work, cautious with external actions
- Internal updates: memory/YYYY-MM-DD.md
- Summary updates: this MEMORY.md file

Group Chat Protocol:
- Participate, don't dominate
- React when truly relevant, thoughtful reactions over quantity
- HEARTBEAT_OK for no action needed
- Quality > quantity

Key People:
- Scott Joshua Hill (Rex / RexMonte): My boss. 35, night shift RN at UCLA Resnick, 7.1+ BTC since 2017, building toward geographic sovereignty + international living. Technical, direct, no fluff. Mac Mini M4 local setup.

System Architecture (Feb 2026):
- Hardware: Mac Mini M4 32GB, 120 GB/s memory bandwidth
- Primary model: moe-fast (Qwen3 30B-A3B MoE, ~20-30 tok/s, only 3B active per token)
- Fast/subagent model: q8-fast (Qwen3 8B, fastest)
- Code model: coder-fast (Qwen3 Coder 30B)
- Balanced model: qwen3-fast (Qwen3 14B)
- All optimized: 8K context, no-think, 4K max output, flash attention, q8_0 KV cache
- API fallbacks: sonnet, haiku, opus (Anthropic)
- Security: sandbox non-main, localhost-only, allowlist groups, log redaction
- Channel: Discord (primary)
- Hardware: Mac Mini M4 32GB, 120 GB/s memory bandwidth

Operations Infrastructure (Feb 2026):
- Mission: maximize Rex's productivity, build passive income pipeline
- Three agent channels in Rex Enterprise Discord (AI agents category):
  - #scout (moe-fast, :00) â€” passive income research
  - #pulse (q8-fast, :40) â€” trend monitoring
  - #forge (coder-fast, :20) â€” MVP scoping/building
- All run on local models via cron, 2h staggered cadence, zero API cost
- Cron job names: ops:scout, ops:pulse, ops:forge
- Channel IDs: scout=1475007773816131635, pulse=1475007798063534192, forge=1475007798881288252

Lessons Learned:
- Thinking mode + large context = compaction death spiral (kills performance)
- 8K context >> 30K for interactive use on local models
- MoE models are the sweet spot for M4: fast as small models, smart as large ones
- Always set num_ctx explicitly â€” Ollama auto-detects too high for RAM tier
- Flash attention + q8_0 KV cache = free performance with no quality loss