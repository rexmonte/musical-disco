## 2026-02-21 — Full Ollama Performance Overhaul

### Problem
- 10-minute response times on local Ollama models (M4 Mac Mini 32GB)
- Root cause: thinking mode + 30K context = compaction death spiral
- Each compaction re-evaluates ~15K tokens, 26 compactions per conversation

### Solution Applied (per Rex's research PDF)

**Environment Variables (persistent in .zshrc + Ollama LaunchAgent plist):**
- OLLAMA_FLASH_ATTENTION=1 (zero quality loss, faster)
- OLLAMA_KV_CACHE_TYPE=q8_0 (halves KV cache memory)
- OLLAMA_NUM_PARALLEL=1 (single KV cache slot)
- OLLAMA_MAX_LOADED_MODELS=1 (no memory splitting)
- OLLAMA_KEEP_ALIVE=30m

**Optimized Modelfiles created (all with 8K ctx, 4K max output, /no_think):**
- `q8-fast` — Qwen3 8B, fastest, great for subagents (~20-30 tok/s)
- `qwen3-fast` — Qwen3 14B, good balance (~8-12 tok/s)
- `moe-fast` — Qwen3 30B-A3B MoE, best quality/speed (~20-30 tok/s, only 3B active)
- `coder-fast` — Qwen3 Coder 30B, specialized for code

**OpenClaw Config:**
- Primary model: `ollama/moe-fast` (MoE — fast + high quality)
- Subagents: `ollama/q8-fast` (fastest for background tasks)
- Aliases: q8, q14, moe, coder (for /model switching)
- Anthropic models kept as fallback aliases (sonnet, haiku, opus)

**Model Role Assignment:**
- Chat/general: moe-fast (default)
- Quick tasks/subagents: q8-fast
- Complex reasoning (on-demand): qwen3-fast (14B)
- Coding: coder-fast

### Key Insight
8K context >> 30K context for interactive use. Manage conversation length at app layer. Fast 8K beats sluggish 30K.
